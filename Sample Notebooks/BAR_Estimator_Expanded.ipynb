{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341f4c2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# WARNING: The functionality herein should be considered experimental and unfinished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import safep\n",
    "import alchemlyb\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from alchemlyb.parsing import namd\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from alchemlyb.estimators import BAR\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae189ca",
   "metadata": {},
   "source": [
    "# User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/path/to/fepouts'\n",
    "filename='*.fepout'\n",
    "temperature = 303.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee77fd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IMPORTANT: Make sure the temperature above matches the temperature used to run the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "import os\n",
    "\n",
    "RT = 0.00198720650096 * temperature\n",
    "\n",
    "fepoutFiles = glob(path+filename)\n",
    "totalSize = 0\n",
    "for file in fepoutFiles:\n",
    "    totalSize += os.path.getsize(file)\n",
    "print(f\"Will process {len(fepoutFiles)} fepout files.\\nTotal size:{np.round(totalSize/10**9, 2)}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3aa52-17f1-44fc-afd8-dfda2787cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSize = 10**9 #Don't use the alchemlyb parser if larger than this size. (bytes)\n",
    "decorrelate = True\n",
    "detectEQ = True\n",
    "\n",
    "#ML = fit PDF of discrepancies with a normal distribution maximum likelihood estimator. \n",
    "#LS = fit CDF of discrepancies with a normal distribution least-squares estimator\n",
    "DiscrepancyFitting = 'LS' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb2766",
   "metadata": {
    "tags": []
   },
   "source": [
    "# For large data sets: read, decorrelate, save\n",
    "This reduces RAM requirements between reading and decorrelating\n",
    "\n",
    "Remember: pickles are not future-proof and should not be used for long-term data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7b01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from alchemlyb.preprocessing import subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345a68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if totalSize>maxSize:\n",
    "    method = 'dE'\n",
    "    affix = f'decorrelated_{method}'\n",
    "\n",
    "    pickles = []\n",
    "    idx = 0\n",
    "\n",
    "    for file in tqdm(fepoutFiles):\n",
    "        df = readFiles([file])\n",
    "        u_nk = u_nk_fromDF(df, temperature, 0, warnings=False)\n",
    "\n",
    "        groups = u_nk.groupby('fep-lambda')\n",
    "        decorr = pd.DataFrame([])\n",
    "        for key, group in groups:\n",
    "            test = subsampling.decorrelate_u_nk(group, method)\n",
    "            decorr = decorr.append(test)\n",
    "        u_nk = decorr\n",
    "        pickle = f\"{path}{affix}{idx:03d}.pkl\"\n",
    "        u_nk.to_pickle(pickle)\n",
    "        pickles.append(pickle)\n",
    "        idx +=1\n",
    "        \n",
    "    pickleDFs = []\n",
    "    for pickle in pickles:\n",
    "        pickleDFs.append(pd.read_pickle(pickle))\n",
    "\n",
    "    u_nk = pd.concat(pickleDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f227d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demonstration of equivalence between the above and below methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cfde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Demonstrate that AFEP readFiles+u_nk_fromDF is identical to namd.extract_u_nk\n",
    "## readFiles is more space efficient and can handle single files. Only reads each file once. Less input validation than alchemlyb.namd.\n",
    "#u_nk_target = namd.extract_u_nk(fepoutFiles[0:5], temperature)\n",
    "#df = readFiles(fepoutFiles[0:5])\n",
    "#u_nk_test = u_nk_fromDF(df, temperature, 10000)\n",
    "#np.all(u_nk_target.fillna(100)==u_nk_test.fillna(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bee8b",
   "metadata": {},
   "source": [
    "# Small data sets can be read and decorrelated sequentially, if desired\n",
    "See Shirts and Chodera (2008) for more details\n",
    "\n",
    "\"Statistically optimal analysis of samples from multiple equilibrium states\" doi: 10.1063/1.2978177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if totalSize < maxSize:\n",
    "    from alchemlyb.preprocessing import subsampling\n",
    "\n",
    "    u_nk = namd.extract_u_nk(fepoutFiles, temperature)\n",
    "    \n",
    "    affix=\"\"\n",
    "    \n",
    "    if detectEQ:\n",
    "        print(\"Detecting Equilibrium\")\n",
    "        affix = f\"{affix}_AutoEquilibrium\"\n",
    "        groups = u_nk.groupby('fep-lambda')\n",
    "        EQ = pd.DataFrame([])\n",
    "        for key, group in groups:\n",
    "            group = group[~group.index.duplicated(keep='first')]\n",
    "            test = subsampling.equilibrium_detection(group, group.dropna(axis=1).iloc[:,-1])\n",
    "            EQ = EQ.append(test)\n",
    "        u_nk = EQ\n",
    "    else:\n",
    "        affix=f\"{affix}_HardEquilibrium\"\n",
    "    \n",
    "    if decorrelate:\n",
    "        print(\"Decorrelating samples\")\n",
    "        method = 'dE'\n",
    "        affix = f'{affix}_decorrelated_{method}'\n",
    "        groups = u_nk.groupby('fep-lambda')\n",
    "        decorr = pd.DataFrame([])\n",
    "        for key, group in groups:\n",
    "            test = subsampling.decorrelate_u_nk(group, method)\n",
    "            decorr = decorr.append(test)\n",
    "        u_nk = decorr\n",
    "    else:\n",
    "        affix = f'{affix}_unprocessed'\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(f\"Warning: The files you are trying to read are quite large. Total size={totalSize}. Try reading and decorrelating (above) or change the maxSize parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea039c3",
   "metadata": {},
   "source": [
    "# Carry out MBAR Fitting and Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d681247-b508-4940-9622-2650864e2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nk = u_nk.sort_index(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002929bf-121d-41b5-aa36-d343878ea65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame()\n",
    "for fl in set(u_nk.index.get_level_values(1)):\n",
    "    counts[fl] = u_nk.loc[(slice(None), fl), :].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15263b7c-5bc0-4c18-8b29-2a165960721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countSeries = pd.Series(np.diagonal(counts.sort_index(axis=1)), index=counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4d941-d417-4969-ad61-08999509a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(countSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a283c3-dd6b-445e-a6c7-4aed623c0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(countSeries)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Fep-lambda')\n",
    "plt.title(f'{affix}_Number of samples per simulated lambda value')\n",
    "plt.savefig(f'{path}{affix}_nSamples.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17c4bf-df1d-46de-94d2-da998a655ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = u_nk.groupby('fep-lambda')\n",
    "lambdas=[key for key, group in groups]\n",
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f64f9-38aa-4f40-b953-16ce4a301795",
   "metadata": {},
   "source": [
    "## Demonstrate an autocorrelation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a911a1b-a6dd-40f3-bf19-82c69923854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "ser = u_nk.loc[(slice(None), lambdas[0]), lambdas[1]]\n",
    "flucs = ser - np.mean(ser)\n",
    "corr = correlate(flucs, flucs, mode='same', method='direct')\n",
    "corr = corr[len(corr)//2:]\n",
    "plt.plot(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30721156",
   "metadata": {},
   "source": [
    "# Extract key features from the MBAR fitting and get Î”G\n",
    "Note: alchemlyb operates in units of kT by default. We multiply by RT to conver to units of kcal/mol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "perWindow, cumulative = safep.do_estimation(u_nk) #Run the BAR estimator on the fep data\n",
    "changeAndError = f'\\u0394G = {np.round(forward[-1]*RT, 1)}\\u00B1{np.round(errors[-1], 3)} kcal/mol'\n",
    "print(changeAndError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51aba3b",
   "metadata": {},
   "source": [
    "# Plot the change in free energy based on MBAR estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac26b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative change in kT\n",
    "plt.errorbar(l, f, yerr=errors, marker='.')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('DeltaG(lambda) (kT)')\n",
    "plt.title(f'Cumulative dG with accumulated errors {affix}\\n{changeAndError}')\n",
    "plt.savefig(f'{path}dG_cumulative_kT_{affix}.png', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# Cumulative change in kcal/mol\n",
    "\"\"\"\n",
    "plt.errorbar(l, f * RT, yerr=errors*RT, marker='.')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('DeltaG(lambda)(kcal/mol)')\n",
    "plt.savefig(f'{path}dG_cumulative_kcal_per_mol_{affix}.png', dpi=600)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Per-window change in kT\n",
    "plt.errorbar(l_mid, df, yerr=ddf, marker='.')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Delta G per window (kT)')\n",
    "plt.title(f'Per-Window dG with individual errors {affix}')\n",
    "plt.savefig(f'{path}dG_{affix}.png', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# Per-window change in kT\n",
    "\n",
    "plt.errorbar(l[1:-1], np.diff(df), marker='.')\n",
    "plt.xlabel('lambda (L)')\n",
    "plt.ylabel(\"ddG(L)\")\n",
    "plt.title(f'derivative of dG {affix}')\n",
    "plt.savefig(f'{path}ddG_{affix}.png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f90d8c",
   "metadata": {},
   "source": [
    "# Plot the estimated total change in free energy as a function of simulation time; contiguous subsets starting at t=0 (\"Forward\") and t=end (\"Reverse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, convAx = plt.subplots(1,1)\n",
    "forward, forward_error, backward, backward_error = safep.do_convergence(u_nk)\n",
    "convAx = safep.convergence_plot(convAx, forward*RT, forward_error*RT, backward*RT, backward_error*RT)\n",
    "plt.savefig(f'{path}FEP_convergence.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948bf74-b3be-4ff5-89aa-849d1d0eff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d264d4b-b7aa-4265-92b5-71fbda2306a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for bootstrapping estimates and generating confidence intervals\n",
    "def bootStrapEstimate(u_nk, estimator='BAR', iterations=100, schedule=[10,20,30,40,50,60,70,80,90,100]):\n",
    "    groups = u_nk.groupby('fep-lambda')\n",
    "\n",
    "    if estimator == 'EXP':\n",
    "        dGfs = {}\n",
    "        dGbs = {}\n",
    "        means = {}\n",
    "    elif estimator == 'BAR':\n",
    "        dGs = {}\n",
    "        errs = {}\n",
    "    else:\n",
    "        raise ValueError(f\"unknown estimator: {estimator}\")\n",
    "\n",
    "    for p in schedule:\n",
    "        Fs = []\n",
    "        Bs = []\n",
    "        fs = []\n",
    "        Gs = []\n",
    "        #rs = []\n",
    "        for i in np.arange(iterations):\n",
    "            sampled = pd.DataFrame([])\n",
    "            for key, group in groups:\n",
    "                N = int(p*len(group)/100)\n",
    "                if N < 1:\n",
    "                    N=1\n",
    "                rows = np.random.choice(len(group), size=N)\n",
    "                test = group.iloc[rows,:]\n",
    "                sampled = sampled.append(test)\n",
    "            if estimator == 'EXP':\n",
    "                l, l_mid, dG_f, dG_b = safep.get_EXP(pd.DataFrame(sampled))\n",
    "                F = np.sum(dG_f)\n",
    "                B = np.sum(-dG_b)\n",
    "                Fs.append(F)\n",
    "                Bs.append(B)\n",
    "                Gs.append(np.mean([F,B]))\n",
    "            elif estimator == 'BAR':\n",
    "                tmpBar = BAR()\n",
    "                tmpBar.fit(sampled)\n",
    "                l, l_mid, f, df, ddf, errors = safep.get_BAR(tmpBar)\n",
    "                fs.append(f.values[-1])\n",
    "                #rs.append(errors[-1])\n",
    "\n",
    "        if estimator == 'EXP':\n",
    "            dGfs[p] = Fs\n",
    "            dGbs[p] = Bs\n",
    "            means[p] = Gs\n",
    "        else:\n",
    "            dGs[p] = fs\n",
    "            #errs[p] = rs\n",
    "\n",
    "    if estimator == 'EXP':\n",
    "        fwd = pd.DataFrame(dGfs).melt().copy()\n",
    "        bwd = pd.DataFrame(dGbs).melt().copy()\n",
    "        alldGs = fwd.append(bwd)\n",
    "        means = pd.DataFrame(means).melt().copy()\n",
    "        return (alldGs, fwd, bwd, means)\n",
    "    else:\n",
    "        alldGs = pd.DataFrame(dGs).melt().copy()\n",
    "        #allErrors = pd.DataFrame(errs).melt().copy()\n",
    "        return alldGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ceb61-966c-43d6-b552-7c385890df97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = 'BAR'\n",
    "iterations = 100\n",
    "schedule = tqdm([1, 10, 25, 50, 75, 100])\n",
    "if estimator == 'BAR':\n",
    "    alldGs = bootStrapEstimate(u_nk, estimator, iterations, schedule)\n",
    "elif estimator == 'EXP':\n",
    "    alldGs, fwd, bwd, means  = bootStrapEstimate(u_nk, estimator, iterations, schedule)\n",
    "else:\n",
    "    raise(f'Error: estimator {estimator} not known')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f4032-81f7-42af-a173-65a5e6b821d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimator=='EXP':\n",
    "    fwd = fwd.set_index('variable')\n",
    "    bwd = bwd.set_index('variable')\n",
    "alldGs = alldGs.set_index('variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84083ef-757a-4e0b-bbf8-090a94ddd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval=0.95\n",
    "upper, lower, means = safep.get_empirical_CI(alldGs, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebc070-7f9b-4a8e-a2ed-f203293dbd3d",
   "metadata": {},
   "source": [
    "upper, lower, means = getLimits(alldGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae9378-043d-4e1a-a040-4b2c378b55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convergence_plot(u_nk, l)\n",
    "#plt.plot(keys, upper, color='gray')\n",
    "#plt.plot(keys, lower, color='gray')\n",
    "keys = np.sort(list(set(alldGs.index)))\n",
    "plt.fill_between(keys/100, upper, lower, color='gray', alpha=0.3, label=f\"{interval*100}% confidence interval\")\n",
    "plt.plot(keys/100, means)\n",
    "if estimator == 'EXP':\n",
    "    plt.scatter(fwd.index/100, fwd.value, label='Forward Samples', s=2)\n",
    "    plt.scatter(bwd.index/100, bwd.value, label='Backward Samples', s=2)\n",
    "else:\n",
    "    plt.scatter(alldGs.index/100, alldGs.value, label='BAR Estimates', s=2)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Net dG (kT)\")\n",
    "plt.xlabel(\"Percent of data used\")\n",
    "\n",
    "plt.title(f'Bootstrapped Estimate: {np.round(means[-1]*RT,1)} +/- {np.round((upper[-1]-means[-1])*RT,1)}kcal/mol')\n",
    "\n",
    "plt.savefig(f'{path}/convergence_{estimator}_{iterations}_{affix}.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a8882-51af-44aa-a77d-8bcf40f00a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 10\n",
    "if estimator == 'EXP':\n",
    "    fvals, fbins, _ = plt.hist(fwd.loc[100], label='forward', bins=nbins, histtype='step')\n",
    "    bvals, bbins, _ = plt.hist(bwd.loc[100], label='backward', bins=nbins, histtype='step')\n",
    "    plt.plot(np.mean([fbins[1:], fbins[:-1]], axis=0), fvals)\n",
    "    plt.plot(np.mean([bbins[1:], bbins[:-1]], axis=0), bvals)\n",
    "    vals, bins, _ = plt.hist(alldGs.loc[100], label='Combined', bins=nbins, histtype='step')\n",
    "    plt.plot(np.mean([bins[1:], bins[:-1]], axis=0), vals)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path}/distribution_of_bootstrapped_samples_{estimator}_{iterations}_{affix}.png', dpi=600)\n",
    "else:\n",
    "    vals, bins, _ = plt.hist(alldGs.loc[100], label='BAR', bins=nbins)\n",
    "    plt.plot(np.mean([bins[1:], bins[:-1]], axis=0), vals)\n",
    "    plt.xlabel('dG in reduced units')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path}/distribution_of_bootstrapped_samples_{estimator}_{iterations}_{affix}.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6709db",
   "metadata": {},
   "source": [
    "# Use an exponential estimator to assess residual discrepancies and check for hysteresis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l, l_mid, dG_f, dG_b = safep.get_exponential(u_nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b501c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyst = dG_f + np.array(dG_b)\n",
    "plt.vlines(l_mid, np.zeros(len(l_mid)), hyst, label=\"fwd - bwd\", linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Fwd-bwd discrepancies by lambda {affix}')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Diff. in delta-G')\n",
    "plt.savefig(f'{path}discrepancies_{affix}.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653946b-5324-4896-be11-f4c4de1fa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_mid, hyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c44db8-3c4e-4915-931c-38d0734c8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "flucs = hyst - np.mean(hyst)\n",
    "corr = correlate(flucs, flucs, mode='full')\n",
    "corr = corr[len(corr)//2:]\n",
    "plt.plot(corr)\n",
    "#plt.savefig(f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9430c7-a886-4ab8-ac50-de42b7953460",
   "metadata": {},
   "source": [
    "# OPTIONAL: Estimate and plot the Cumulative Density function (CDF) for the differences shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6556861-71ed-46ac-90f9-6ce63c700662",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = dG_f + np.array(dG_b)\n",
    "diff.sort()\n",
    "X = diff\n",
    "Y = np.arange(len(X))/len(X)\n",
    "\n",
    "#fit a normal distribution to the existing data\n",
    "#fitted = norm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0a342-631f-41f2-9924-6efb2878c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "from scipy.optimize import curve_fit as scipyFit\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "\n",
    "#Wrapper for fitting the normal CDF\n",
    "def cumFn(x, m, s):\n",
    "    r = norm.cdf(x, m, s)\n",
    "    return r\n",
    "\n",
    "def pdfFn(x,m,s):\n",
    "    r = norm.pdf(x,m,s)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88943b75-4c3f-4149-a58a-9284fff9957f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fit a normal distribution to the existing data\n",
    "\n",
    "if DiscrepancyFitting == 'LS':\n",
    "    fitted = scipyFit(cumFn, X, Y)[0] #Fit norm.cdf to (X,Y)\n",
    "elif DiscrepancyFitting == 'ML':\n",
    "    #fitted = scipyFit(pdfFn, X, Y)[0]\n",
    "    fitted = norm.fit(X) # fit a normal distribution to X\n",
    "else:\n",
    "    raise(\"Error: Discrepancy fitting code not known. Acceptable values: ML or LS\")\n",
    "discrepancies = dG_f + np.array(dG_b)\n",
    "\n",
    "dx = 0.01\n",
    "cdfXnorm  = np.arange(np.min(X), np.max(X), dx)\n",
    "cdfYnorm = norm.cdf(cdfXnorm, fitted[0], fitted[1])\n",
    "cdfYexpected = norm.cdf(X, fitted[0], fitted[1])\n",
    "\n",
    "#plot the data as they are (estimate the CDF) and the fitted cdf\n",
    "fig, (cdfAx, cdfResid, pdfAx, pdfResid) = plt.subplots(4, 1, sharex=True)\n",
    "plt.xlabel('Difference in delta-G')\n",
    "\n",
    "cdfAx.scatter(X, Y, 2, label=\"Fwd bkd differences\")\n",
    "cdfAx.plot(cdfXnorm, cdfYnorm, label=\"Normal Distribution\", color=\"orange\")\n",
    "cdfAx.set_ylabel(\"CDF\")\n",
    "cdfAx.legend()\n",
    "\n",
    "#cdf Residuals\n",
    "cdfResiduals = Y-cdfYexpected\n",
    "cdfResid.plot(X, cdfResiduals)\n",
    "cdfResid.set_ylabel(\"CDF residuals\")\n",
    "\n",
    "#pdf\n",
    "dx = 0.01\n",
    "dx = 0.01\n",
    "\n",
    "binNum = 20\n",
    "window = binNum\n",
    "pdfY, pdfX = np.histogram(discrepancies, bins=binNum, density=True)\n",
    "pdfX = (pdfX[1:]+pdfX[:-1])/2\n",
    "\n",
    "pdfXnorm  = np.arange(np.min(X), np.max(X), dx)\n",
    "pdfYnorm = norm.pdf(pdfXnorm, fitted[0], fitted[1])\n",
    "\n",
    "pdfYexpected = norm.pdf(pdfX, fitted[0], fitted[1])\n",
    "\n",
    "pdfAx.plot(pdfX, pdfY,  label=\"Estimated Distribution\")\n",
    "pdfAx.set_ylabel(\"PDF\")\n",
    "pdfAx.plot(pdfXnorm, pdfYnorm, label=\"Fitted Normal Distribution\", color=\"orange\")\n",
    "\n",
    "#pdf residuals\n",
    "pdfResiduals = pdfY-pdfYexpected\n",
    "pdfResid.plot(pdfX, pdfResiduals)\n",
    "pdfResid.set_ylabel(\"PDF residuals\") \n",
    "\n",
    "\n",
    "fig.set_figheight(10)\n",
    "if DiscrepancyFitting == 'LS':\n",
    "    cdfAx.title.set_text(f\"Least squares fitting of cdf(fwd-bkwd)\\nSkewness: {np.round(skew(X),2)}\\nFitted parameters: Mean={np.round(fitted[0],3)}, Stdv={np.round(fitted[1],3)}\\nPopulation parameters: Mean={np.round(np.average(X),3)}, Stdv={np.round(np.std(X),3)}\")\n",
    "    plt.savefig(f\"{path}LeastSquaresCDF{affix}.png\", dpi=600)\n",
    "elif DiscrepancyFitting == 'ML':\n",
    "    cdfAx.title.set_text(f\"Maximum likelihood fitting of fwd-bkwd\\nFitted parameters: Mean={np.round(fitted[0],3)}, Stdv={np.round(fitted[1],3)}\\nPopulation parameters: Mean={np.round(np.average(X),3)}, Stdv={np.round(np.std(X),3)}\")\n",
    "    plt.savefig(f\"{path}MaximumLikelihood{affix}.png\", dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60caa0-e27d-4a1d-98aa-d475364de9a5",
   "metadata": {},
   "source": [
    "# OPTIONAL DIAGNOSTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e15321-ec66-446e-bb59-1b73e8a3dca3",
   "metadata": {},
   "source": [
    "# For looking at FEP data as a time series. Can be useful diagnostically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f883b1c-6714-4394-8871-d44f5801ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardMask = u_nk.copy()\n",
    "offset = u_nk.columns[1] - u_nk.columns[0]\n",
    "\n",
    "for col in forwardMask.columns:\n",
    "    forwardMask[col].values[:] = 0\n",
    "\n",
    "for x in set(u_nk.index.get_level_values(1)):\n",
    "    forwardMask.loc[(slice(None),x),np.round(x+offset, 3)] = True\n",
    "\n",
    "forwardMask = forwardMask.dropna(axis=1)\n",
    "\n",
    "backwardMask = u_nk.copy()\n",
    "offset = u_nk.columns[1] - u_nk.columns[0]\n",
    "\n",
    "for col in backwardMask.columns:\n",
    "    backwardMask[col].values[:] = 0\n",
    "\n",
    "for x in set(u_nk.index.get_level_values(1)):\n",
    "    backwardMask.loc[(slice(None),x),np.round(x+offset, 3)] = True\n",
    "\n",
    "backwardMask = backwardMask.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f15ae-53a2-427e-a49f-8f4df1d435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nk.mask(forwardMask.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce074ce-b1fe-4191-ab67-e34abe15d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.0000\n",
    "l2 = 0.0125\n",
    "forward = u_nk.loc[(slice(None), l), l2]\n",
    "plt.plot(forward.index.get_level_values(0), forward, label=\"forward\")\n",
    "reverse = -1*u_nk.loc[(slice(None), l2), l]\n",
    "plt.plot(reverse.index.get_level_values(0), reverse, label=\"reverse\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a13340-251e-475c-b649-13356aeeba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "histF, edgesF = np.histogram(forward, density=True)\n",
    "histR, edgesR = np.histogram(reverse, density=True)\n",
    "\n",
    "plt.plot(np.mean([edgesF[1:], edgesF[:-1]], axis=0), histF, label =\"forward\")\n",
    "plt.plot(np.mean([edgesR[1:], edgesR[:-1]], axis=0), histR, label=\"backward\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d3d02-6cc1-413e-b151-71cc40626446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(l_mid, dG_f)\n",
    "plt.plot(l_mid, -dG_b)\n",
    "plt.title(f\"fwd and -bwd exponential estimations {affix}\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"dG\")\n",
    "plt.savefig(f\"{path}exponentials.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402caa17-f217-4413-af60-e698ee892601",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_mid, np.cumsum(dG_f))\n",
    "plt.plot(l_mid, -np.cumsum(dG_b))\n",
    "plt.title(f\"fwd and -bwd exponential estimations {affix}\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"dG\")\n",
    "plt.savefig(f\"{path}exponentials_cumulative_{affix}.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6bb166-c233-4f48-9f55-e5f61294786a",
   "metadata": {},
   "source": [
    "# Using the exponential estimator to estimate an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8ec25-5e22-4099-bb2d-c2ed54e337bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f593d40-8d97-43f7-ab54-aad8a2832c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in np.arange(len(dG_f)):\n",
    "    errors.append(scipy.stats.sem([dG_f[i], -dG_b[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acba20-2eae-4bb2-91a1-e136936ef119",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.linalg.norm(errors, ord=2)\n",
    "\n",
    "print(f\"The estimated cumulative error using the exponential estimator is: {np.round(err*RT,3)} kcal/mol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb664f9-e0a6-43ca-bc75-a3902a5dbaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
