{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341f4c2c",
   "metadata": {},
   "source": [
    "# Import AFEP_parse and dependencies\n",
    "AFEP_parse.py contains all the functions and library calls necessary to run the notebook\n",
    "\n",
    "# Required modules:\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- alchemlyb (`pip install git+https://github.com/alchemistry/alchemlyb`)\n",
    "- natsort (for sorting file names)\n",
    "- glob (for unix-like file paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AFEP_parse import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae189ca",
   "metadata": {},
   "source": [
    "# User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/u2/home_u2/ems363/Documents/ELIC_DCDs_Analyses/PCPG31/POCG_14A/'\n",
    "filename='PO*.fepout'\n",
    "\n",
    "temperature = 303.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee77fd0",
   "metadata": {},
   "source": [
    "## IMPORTANT: Make sure the temperature above matches the temperature used to run the simulations.\n",
    "\n",
    "## No editing needed beyond this point\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "import os\n",
    "\n",
    "RT = 0.00198720650096 * temperature\n",
    "fepoutFiles = glob(path+filename)\n",
    "totalSize = 0\n",
    "for file in fepoutFiles:\n",
    "    totalSize += os.path.getsize(file)\n",
    "print(f\"Will process {len(fepoutFiles)} fepout files.\\nTotal size:{np.round(totalSize/10**9, 2)}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3aa52-17f1-44fc-afd8-dfda2787cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fepoutFiles = natsorted(fepoutFiles)\n",
    "maxSize = 10**9 #Don't use the alchemlyb parser if larger than this size. (bytes)\n",
    "decorrelate = True\n",
    "detectEQ = False\n",
    "DiscrepancyFitting = 'LS' #ML = fit PDF of discrepancies with a normal distribution maximum likelihood estimator. LS = fit CDF of discrepancies with a normal distribution least-squares estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb2766",
   "metadata": {},
   "source": [
    "# For large data sets: read, decorrelate, save\n",
    "This reduces RAM requirements between reading and decorrelating\n",
    "\n",
    "Remember: pickles are not future-proof and should not be used for long-term data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alchemlyb.preprocessing import subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if totalSize>maxSize:\n",
    "    method = 'dE'\n",
    "    affix = f'decorrelated_{method}'\n",
    "\n",
    "    pickles = []\n",
    "    idx = 0\n",
    "\n",
    "    for file in tqdm(fepoutFiles):\n",
    "        df = readFiles([file])\n",
    "        u_nk = u_nk_fromDF(df, temperature, 0, warnings=False)\n",
    "\n",
    "        groups = u_nk.groupby('fep-lambda')\n",
    "        decorr = pd.DataFrame([])\n",
    "        for key, group in groups:\n",
    "            test = subsampling.decorrelate_u_nk(group, method)\n",
    "            decorr = decorr.append(test)\n",
    "        u_nk = decorr\n",
    "        pickle = f\"{path}{affix}{idx:03d}.pkl\"\n",
    "        u_nk.to_pickle(pickle)\n",
    "        pickles.append(pickle)\n",
    "        idx +=1\n",
    "        \n",
    "    pickleDFs = []\n",
    "    for pickle in pickles:\n",
    "        pickleDFs.append(pd.read_pickle(pickle))\n",
    "\n",
    "    u_nk = pd.concat(pickleDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_nk = u_nk.drop(0.975, axis=1) #to remove incomplete windows e.g. when changing lambda resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f227d8",
   "metadata": {},
   "source": [
    "# Demonstration of equivalence between the above and below methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cfde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate that AFEP readFiles+u_nk_fromDF is identical to namd.extract_u_nk\n",
    "## readFiles is more space efficient and can handle single files. Only reads each file once. Less input validation than alchemlyb.namd.\n",
    "#u_nk_target = namd.extract_u_nk(fepoutFiles[0:5], temperature)\n",
    "#df = readFiles(fepoutFiles[0:5])\n",
    "#u_nk_test = u_nk_fromDF(df, temperature, 10000)\n",
    "#np.all(u_nk_target.fillna(100)==u_nk_test.fillna(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bee8b",
   "metadata": {},
   "source": [
    "# Small data sets can be read and decorrelated sequentially, if desired\n",
    "See Shirts and Chodera (2008) for more details\n",
    "\n",
    "\"Statistically optimal analysis of samples from multiple equilibrium states\" doi: 10.1063/1.2978177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if totalSize < maxSize:\n",
    "    from alchemlyb.preprocessing import subsampling\n",
    "\n",
    "    u_nk = namd.extract_u_nk(fepoutFiles, temperature)\n",
    "    \n",
    "    affix=\"\"\n",
    "    \n",
    "    if decorrelate:\n",
    "        print(\"Decorrelating samples\")\n",
    "        method = 'dE'\n",
    "        affix = f'{affix}_decorrelated_{method}'\n",
    "        groups = u_nk.groupby('fep-lambda')\n",
    "        decorr = pd.DataFrame([])\n",
    "        for key, group in groups:\n",
    "            test = subsampling.decorrelate_u_nk(group, method)\n",
    "            decorr = decorr.append(test)\n",
    "        u_nk = decorr\n",
    "    else:\n",
    "        affix = f'{affix}_unprocessed'\n",
    "    \n",
    "    if detectEQ:\n",
    "        print(\"Detecting Equilibrium\")\n",
    "        affix = f\"{affix}_AutoEquilibrium\"\n",
    "        groups = u_nk.groupby('fep-lambda')\n",
    "        EQ = pd.DataFrame([])\n",
    "        for key, group in groups:\n",
    "            group = group[~group.index.duplicated(keep='first')]\n",
    "            test = subsampling.equilibrium_detection(group, group.dropna(axis=1).iloc[:,-1])\n",
    "            EQ = EQ.append(test)\n",
    "        u_nk = EQ\n",
    "    else:\n",
    "        affix=f\"{affix}_HardEquilibrium\"\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"Warning: The files you are trying to read are quite large. Total size={totalSize}. Try reading and decorrelating (above) or change the maxSize parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea039c3",
   "metadata": {},
   "source": [
    "# Carry out MBAR Fitting and Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d681247-b508-4940-9622-2650864e2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nk = u_nk.sort_index(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002929bf-121d-41b5-aa36-d343878ea65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame()\n",
    "for fl in set(u_nk.index.get_level_values(1)):\n",
    "    counts[fl] = u_nk.loc[(slice(None), fl), :].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15263b7c-5bc0-4c18-8b29-2a165960721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countSeries = pd.Series(np.diagonal(counts.sort_index(axis=1)), index=counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4d941-d417-4969-ad61-08999509a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(countSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a283c3-dd6b-445e-a6c7-4aed623c0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(countSeries)\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Fep-lambda')\n",
    "plt.title(f'{affix}_Number of samples per simulated lambda value')\n",
    "plt.savefig(f'{path}{affix}_nSamples.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = BAR()\n",
    "bar.fit(u_nk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30721156",
   "metadata": {},
   "source": [
    "# Extract key features from the MBAR fitting and get Î”G\n",
    "Note: alchemlyb operates in units of kT by default. We multiply by RT to conver to units of kcal/mol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "l, l_mid, f, df, ddf, errors = get_BAR(bar)\n",
    "changeAndError = f'\\u0394G = {np.round(f.iloc[-1]*RT, 1)}\\u00B1{np.round(errors[-1], 3)} kcal/mol'\n",
    "print(changeAndError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51aba3b",
   "metadata": {},
   "source": [
    "# Plot the change in free energy based on MBAR estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac26b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative change in kT\n",
    "plt.errorbar(l, f, yerr=errors, marker='.')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('DeltaG(lambda) (kT)')\n",
    "plt.title(f'Cumulative dG with accumulated errors {affix}\\n{changeAndError}')\n",
    "plt.savefig(f'{path}dG_cumulative_kT_{affix}.png', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# Cumulative change in kcal/mol\n",
    "\"\"\"\n",
    "plt.errorbar(l, f * RT, yerr=errors*RT, marker='.')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('DeltaG(lambda)(kcal/mol)')\n",
    "plt.savefig(f'{path}dG_cumulative_kcal_per_mol_{affix}.png', dpi=600)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Per-window change in kT\n",
    "plt.errorbar(l_mid, df, yerr=ddf, marker='.')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Delta G per window (kT)')\n",
    "plt.title(f'Per-Window dG with individual errors {affix}')\n",
    "plt.savefig(f'{path}dG_{affix}.png', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# Per-window change in kT\n",
    "\n",
    "plt.errorbar(l[1:-1], np.diff(df), marker='.')\n",
    "plt.xlabel('lambda (L)')\n",
    "plt.ylabel(\"dG'(L)\")\n",
    "plt.title(f'derivative of dG {affix}')\n",
    "plt.savefig(f'{path}dG_prime_{affix}.png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f90d8c",
   "metadata": {},
   "source": [
    "# Plot the estimated total change in free energy as a function of simulation time; contiguous subsets starting at t=0 (\"Forward\") and t=end (\"Reverse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_plot(u_nk, l)\n",
    "plt.title(f'Convergence {affix}')\n",
    "plt.savefig(f'{path}convergence_{affix}.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948bf74-b3be-4ff5-89aa-849d1d0eff60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6709db",
   "metadata": {},
   "source": [
    "# Use an exponential estimator to assess residual discrepancies and check for hysteresis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l, l_mid, dG_f, dG_b = get_EXP(u_nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b501c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.vlines(l_mid, np.zeros(len(l_mid)), dG_f + np.array(dG_b), label=\"fwd - bwd\", linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Fwd-bwd discrepancies by lambda {affix}')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Diff. in delta-G')\n",
    "plt.savefig(f'{path}discrepancies_{affix}.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9430c7-a886-4ab8-ac50-de42b7953460",
   "metadata": {},
   "source": [
    "# OPTIONAL: Estimate and plot the Cumulative Density function (CDF) for the differences shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6556861-71ed-46ac-90f9-6ce63c700662",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = dG_f + np.array(dG_b)\n",
    "diff.sort()\n",
    "X = diff\n",
    "Y = np.arange(len(X))/len(X)\n",
    "\n",
    "#fit a normal distribution to the existing data\n",
    "#fitted = norm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0a342-631f-41f2-9924-6efb2878c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "from scipy.optimize import curve_fit as scipyFit\n",
    "from scipy.stats import skew\n",
    "#Wrapper for fitting the normal CDF\n",
    "def cumFn(x, m, s):\n",
    "    r = norm.cdf(x, m, s)\n",
    "    return r\n",
    "\n",
    "def pdfFn(x,m,s):\n",
    "    r = norm.pdf(x,m,s)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88943b75-4c3f-4149-a58a-9284fff9957f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fit a normal distribution to the existing data\n",
    "\n",
    "if DiscrepancyFitting == 'LS':\n",
    "    fitted = scipyFit(cumFn, X, Y)[0] #Fit norm.cdf to (X,Y)\n",
    "elif DiscrepancyFitting == 'ML':\n",
    "    #fitted = scipyFit(pdfFn, X, Y)[0]\n",
    "    fitted = norm.fit(X) # fit a normal distribution to X\n",
    "else:\n",
    "    raise(\"Error: Discrepancy fitting code not known. Acceptable values: ML or LS\")\n",
    "discrepancies = dG_f + np.array(dG_b)\n",
    "\n",
    "dx = 0.01\n",
    "cdfXnorm  = np.arange(np.min(X), np.max(X), dx)\n",
    "cdfYnorm = norm.cdf(cdfXnorm, fitted[0], fitted[1])\n",
    "cdfYexpected = norm.cdf(X, fitted[0], fitted[1])\n",
    "\n",
    "#plot the data as they are (estimate the CDF) and the fitted cdf\n",
    "fig, (cdfAx, cdfResid, pdfAx, pdfResid) = plt.subplots(4, 1, sharex=True)\n",
    "plt.xlabel('Difference in delta-G')\n",
    "\n",
    "cdfAx.scatter(X, Y, 2, label=\"Fwd bkd differences\")\n",
    "cdfAx.plot(cdfXnorm, cdfYnorm, label=\"Normal Distribution\", color=\"orange\")\n",
    "cdfAx.set_ylabel(\"CDF\")\n",
    "cdfAx.legend()\n",
    "\n",
    "#cdf Residuals\n",
    "cdfResiduals = Y-cdfYexpected\n",
    "cdfResid.plot(X, cdfResiduals)\n",
    "cdfResid.set_ylabel(\"CDF residuals\")\n",
    "\n",
    "#pdf\n",
    "dx = 0.01\n",
    "dx = 0.01\n",
    "\n",
    "binNum = 20\n",
    "window = binNum\n",
    "pdfY, pdfX = np.histogram(discrepancies, bins=binNum, density=True)\n",
    "pdfX = (pdfX[1:]+pdfX[:-1])/2\n",
    "\n",
    "pdfXnorm  = np.arange(np.min(X), np.max(X), dx)\n",
    "pdfYnorm = norm.pdf(pdfXnorm, fitted[0], fitted[1])\n",
    "\n",
    "pdfYexpected = norm.pdf(pdfX, fitted[0], fitted[1])\n",
    "\n",
    "pdfAx.plot(pdfX, pdfY,  label=\"Estimated Distribution\")\n",
    "pdfAx.set_ylabel(\"PDF\")\n",
    "pdfAx.plot(pdfXnorm, pdfYnorm, label=\"Fitted Normal Distribution\", color=\"orange\")\n",
    "\n",
    "#pdf residuals\n",
    "pdfResiduals = pdfY-pdfYexpected\n",
    "pdfResid.plot(pdfX, pdfResiduals)\n",
    "pdfResid.set_ylabel(\"PDF residuals\") \n",
    "\n",
    "\n",
    "fig.set_figheight(10)\n",
    "if DiscrepancyFitting == 'LS':\n",
    "    cdfAx.title.set_text(f\"Least squares fitting of cdf(fwd-bkwd)\\nSkewness: {np.round(skew(X),2)}\\nFitted parameters: Mean={np.round(fitted[0],3)}, Stdv={np.round(fitted[1],3)}\\nPopulation parameters: Mean={np.round(np.average(X),3)}, Stdv={np.round(np.std(X),3)}\")\n",
    "    plt.savefig(f\"{path}LeastSquaresCDF{affix}.png\", dpi=600)\n",
    "elif DiscrepancyFitting == 'ML':\n",
    "    cdfAx.title.set_text(f\"Maximum likelihood fitting of fwd-bkwd\\nFitted parameters: Mean={np.round(fitted[0],3)}, Stdv={np.round(fitted[1],3)}\\nPopulation parameters: Mean={np.round(np.average(X),3)}, Stdv={np.round(np.std(X),3)}\")\n",
    "    plt.savefig(f\"{path}MaximumLikelihood{affix}.png\", dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60caa0-e27d-4a1d-98aa-d475364de9a5",
   "metadata": {},
   "source": [
    "# OPTIONAL DIAGNOSTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e15321-ec66-446e-bb59-1b73e8a3dca3",
   "metadata": {},
   "source": [
    "# For looking at FEP data as a time series. Can be useful diagnostically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f883b1c-6714-4394-8871-d44f5801ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardMask = u_nk.copy()\n",
    "offset = u_nk.columns[1] - u_nk.columns[0]\n",
    "\n",
    "for col in forwardMask.columns:\n",
    "    forwardMask[col].values[:] = 0\n",
    "\n",
    "for x in set(u_nk.index.get_level_values(1)):\n",
    "    forwardMask.loc[(slice(None),x),np.round(x+offset, 3)] = True\n",
    "\n",
    "forwardMask = forwardMask.dropna(axis=1)\n",
    "\n",
    "backwardMask = u_nk.copy()\n",
    "offset = u_nk.columns[1] - u_nk.columns[0]\n",
    "\n",
    "for col in backwardMask.columns:\n",
    "    backwardMask[col].values[:] = 0\n",
    "\n",
    "for x in set(u_nk.index.get_level_values(1)):\n",
    "    backwardMask.loc[(slice(None),x),np.round(x+offset, 3)] = True\n",
    "\n",
    "backwardMask = backwardMask.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f15ae-53a2-427e-a49f-8f4df1d435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nk.mask(forwardMask.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce074ce-b1fe-4191-ab67-e34abe15d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.round(0.025*11, 3)\n",
    "l2 = np.round(0.025*12, 3)\n",
    "forward = u_nk.loc[(slice(None), l), l2]\n",
    "plt.plot(forward.index.get_level_values(0), forward, label=\"forward\")\n",
    "reverse = -1*u_nk.loc[(slice(None), l2), l]\n",
    "plt.plot(reverse.index.get_level_values(0), reverse, label=\"reverse\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a13340-251e-475c-b649-13356aeeba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "histF, edgesF = np.histogram(forward, density=True)\n",
    "histR, edgesR = np.histogram(reverse, density=True)\n",
    "\n",
    "plt.plot(np.mean([edgesF[1:], edgesF[:-1]], axis=0), histF, label =\"forward\")\n",
    "plt.plot(np.mean([edgesR[1:], edgesR[:-1]], axis=0), histR, label=\"backward\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d3d02-6cc1-413e-b151-71cc40626446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(l_mid, dG_f)\n",
    "plt.plot(l_mid, -dG_b)\n",
    "plt.title(f\"fwd and -bwd exponential estimations {affix}\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"dG\")\n",
    "plt.savefig(f\"{path}exponentials.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402caa17-f217-4413-af60-e698ee892601",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_mid, np.cumsum(dG_f))\n",
    "plt.plot(l_mid, -np.cumsum(dG_b))\n",
    "plt.title(f\"fwd and -bwd exponential estimations {affix}\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"dG\")\n",
    "plt.savefig(f\"{path}exponentials_cumulative_{affix}.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8a5af-e8ec-4cce-9965-416fad6dd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the exponential estimator to estimate an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8ec25-5e22-4099-bb2d-c2ed54e337bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f593d40-8d97-43f7-ab54-aad8a2832c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in np.arange(len(dG_f)):\n",
    "    errors.append(scipy.stats.sem([dG_f[i], -dG_b[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acba20-2eae-4bb2-91a1-e136936ef119",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(errors, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2c67e-b2fb-4e4e-9533-1f9d8516dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for x in errors:\n",
    "    total += x**2\n",
    "np.sqrt(total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
